{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Photometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Author: *Valentin Christiaens* (Core Nb), Dominique Sluse  \n",
    "> Package dependencies: astropy, matplotlib, numpy, photutils, scikit-image, scipy\n",
    ">\n",
    "> The data needed for this Nb should be downloaded separately along this link: [https://dox.uliege.be/index.php/s/rVCiGqm6oCQV4Pd](https://dox.uliege.be/index.php/s/rVCiGqm6oCQV4Pd)\n",
    "> The data should be moved into the sub-directory `data` (i.e. child directory of the folder hosting this Nb) \n",
    "> \n",
    "> Last update: *2026/02/13*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "* [1. Aperture photometry of point sources](#1.-Aperture-photometry-of-point-sources)\n",
    "    - [1.1. General procedure](#1.1.-General-procedure)\n",
    "    - [1.2. Apertures](#1.2.-Apertures)\n",
    "    - [1.3. Error estimation](#1.3.-Error-estimation)\n",
    "    - [1.4. Aperture photometry with sky coordinates](#1.4.-Aperture-photometry-with-sky-coordinates)\n",
    "* [2. Background subtraction](#2.-Background-subtraction)\n",
    "    - [2.1. Constant value](#2.1.-Constant-value)\n",
    "    - [2.2. Annular estimate](#2.2.-Annular-estimation)\n",
    "    - [2.3. Gradient estimate](#2.3.-Gradient-estimate)\n",
    "* [3. Source identification](#3.-Source-identification)\n",
    "    - [3.1. Centroid measurement](#3.1.-Centroid-measurement)\n",
    "    - [3.2. Image alignment](#3.2.-Image-alignment)\n",
    "    - [3.3. Automatic source finding](#3.3.-Automatic-source-finding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we will see how to tackle the common task of measuring the photometry (i.e. quantify the brightness) of point sources in astronomical images.\n",
    "\n",
    "Before the advent of gravitational wave astronomy, *all* we knew about our Universe came from measuring the light of celestial objects. Chronologically: measurements were first made by eye (through an eyepiece after the invention of the telescope), then through photographic plates placed in the focal plane of the telescope, and more recently through electronic detectors (charge-coupled devices; CCD). \n",
    "\n",
    "Different instruments can be placed in the focal plane of a telescope to count photons, i.e. to perform **photometry**. Imagers (i.e. camera/detectors) provide the image of a field in which a number of sources may be located, and with source contributing the photon counts of different **pixels** in the image. \n",
    "Spectrographs are instruments dispersing the light coming from a given field-of-view before it hits the detector, and can come with various designs.\n",
    "Some spectrograph designs enable the simultaneous acquisition of multi-wavelength images (e.g. integral field spectrographs), where the task of measuring photons from given sources is then referred to as **spectro-photometry**.\n",
    "\n",
    "Note that high-spectral resolution spectrographs (e.g. Echelle spectrographs) typically do not provide spatial information - all the incoming light is dispersed. The outcome then takes the form of a 1D spectrum for the observed object/field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals of this lecture:** \n",
    "\n",
    "1. Be able to measure the photometry of point sources.\n",
    "2. Be able to estimate and subtract the background contribution.\n",
    "3. Know how to identify sources and measure their centroid location.\n",
    "4. Know how to register observed images before stacking them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import modules we will use a lot in this tutorial:\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Aperture photometry of point sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most direct way to quantify the brightness of sources in astronomical images is to perform **aperture photometry**, that is to count and sum the number of photon hits on pixels within a given aperture encompassing the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the aperture should be well chosen to either capture most of the flux of the star, or a well-characterized fraction of the whole incoming flux.\n",
    "\n",
    "The package offering the most options in that purpose is the Astropy-affiliated package `photutils`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `aperture_photometry()` function and the `ApertureStats` class are the main tools to perform aperture photometry on an astronomical image for a given set of apertures.\n",
    "\n",
    "Photutils provides several apertures defined in pixel or sky coordinates. The aperture classes that are defined in pixel coordinates are:\n",
    "\n",
    "- `CircularAperture`\n",
    "- `CircularAnnulus`\n",
    "- `EllipticalAperture`\n",
    "- `EllipticalAnnulus`\n",
    "- `RectangularAperture`\n",
    "- `RectangularAnnulus`\n",
    "\n",
    "Each of these classes has a corresponding variant defined in sky coordinates:\n",
    "\n",
    "- `SkyCircularAperture`\n",
    "- `SkyCircularAnnulus`\n",
    "- `SkyEllipticalAperture`\n",
    "- `SkyEllipticalAnnulus`\n",
    "- `SkyRectangularAperture`\n",
    "- `SkyRectangularAnnulus`\n",
    "\n",
    "To perform aperture photometry with sky-based apertures, one needs to specify a WCS transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. General procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in performing aperture photometry with `photutils` is to create an **aperture object**. An aperture object is defined by a position (or a list of positions) and parameters that define its size and possibly, orientation (e.g., for an elliptical aperture).\n",
    "\n",
    "**Example**: Let's create a circular aperture in pixel coordinates using the `CircularAperture` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.aperture import CircularAperture\n",
    "\n",
    "aperture = CircularAperture((30, 30), r=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the aperture object is created, we can perform the photometry using the `aperture_photometry()` function, providing it both the data and the aperture(s).\n",
    "\n",
    "**Example:** Let's consider an array of all ones and integrate the flux over the apertures defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.aperture import aperture_photometry\n",
    "\n",
    "data = np.ones((100, 100))\n",
    "phot_table = aperture_photometry(data, aperture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phot_table['aperture_sum'].info.format = '%.8g'  # for consistent table output\n",
    "print(phot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the results of the photometry in an Astropy **QTable**. In this example, the table has four columns, named 'id', 'xcenter', 'ycenter', and 'aperture_sum'.\n",
    "\n",
    "Since all the data values are 1.0, the aperture sums are equal to the area of a circle with a radius of 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.pi * 3.0** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Apertures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Photutils provides several apertures defined in pixel or sky coordinates. The aperture classes that are defined in pixel coordinates are:\n",
    "\n",
    "- `CircularAperture`\n",
    "- `CircularAnnulus`\n",
    "- `EllipticalAperture`\n",
    "- `EllipticalAnnulus`\n",
    "- `RectangularAperture`\n",
    "- `RectangularAnnulus`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we want to consider several apertures at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.aperture import CircularAperture\n",
    "\n",
    "positions = [(30.0, 30.0), (40.0, 40.0)]\n",
    "apertures = CircularAperture(positions, r=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positions should be either a single tuple of (x, y) - as seen in the previous section - or a list of (x, y) tuples, or an array with shape Nx2, where N is the number of positions.\n",
    "\n",
    "The above example defines two circular apertures with a radius of 3 pixels, and located at pixel coordinates (30, 30) and (40, 40)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phot_table = aperture_photometry(data, apertures)\n",
    "print(phot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also consider a list of apertures, for example with **multiple aperture sizes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = [3.0, 4.0, 5.0]\n",
    "apertures = [CircularAperture(positions, r=r) for r in radii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phot_table = aperture_photometry(data, apertures)\n",
    "print(phot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-warning\"> \n",
    "**Exercise 1.1:** Measure the aperture photometry of star HD 135344 B ('data/SPHERE_PSF_HD135.fits') in detector units for different, increasing, aperture sizes (from 1 to 80 pixels by steps of 1). Make a plot of the measured flux as a function of aperture size, and estimate the aperture radius which encompasses half of the total energy.\n",
    "\n",
    "Tip: here it is fine to consider the coordinates of the peak intensity pixel for the center of the aperture.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "image = fits.getdata('data/SPHERE_PSF_HD135.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "plt.imshow(image, norm=LogNorm(vmin=1, vmax=np.max(image))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_yx = np.argmax(image)\n",
    "print(max_yx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cy, cx = np.unravel_index(max_yx, image.shape)\n",
    "cy, cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = range(1, 81)\n",
    "position = (cx, cy)\n",
    "apertures = [CircularAperture(position, r=r) for r in radii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_table = aperture_photometry(image, apertures)\n",
    "phot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxes = [float(phot_table['aperture_sum_{}'.format(r-1)][0]) for r in radii]\n",
    "plt.plot(radii, fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_energy = [fluxes[i]/fluxes[-1] for i in range(len(fluxes))]\n",
    "enc_energy = 100*np.array(enc_energy)\n",
    "plt.plot(radii, enc_energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the closest aperture to 50%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii[np.argmin(np.abs(enc_energy-50))] # this radius (in  pixels) contains 50% of the total flux "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to find that radius \n",
    "id_50 = np.searchsorted(enc_energy, 50, side='left')\n",
    "radii[id_50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_energy[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternative** (shortcut) method using `CurveOfGrowth` to plot the profile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.profiles import CurveOfGrowth\n",
    "radii = np.arange(1, 81)\n",
    "cog = CurveOfGrowth(image, position, radii)\n",
    "\n",
    "cog.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Uncertainty estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If and only if the `error` keyword is input to `aperture_photometry()`, the returned table will include a 'aperture_sum_err' column in addition to 'aperture_sum'. 'aperture_sum_err' provides the propagated uncertainty associated with 'aperture_sum'. The error is has to be an array representing the $1 \\sigma$ Gaussian uncertainty on the prixels in the images. \n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [(30.0, 30.0), (40.0, 40.0)]\n",
    "aperture = CircularAperture(positions, r=3.0)\n",
    "data = np.ones((100, 100))\n",
    "error = 0.1 * data  # Error map - Arbitrary value to illustrate how to use it in aperture photometry\n",
    "\n",
    "phot_table = aperture_photometry(data, aperture, error=error)\n",
    "for col in phot_table.colnames:\n",
    "    phot_table[col].info.format = '%.8g'  # for consistent table output: force 8 significant digits\n",
    "print(phot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Aperture photometry with sky coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous to detector-based apertures, one can also define apertures in sky coordinates:\n",
    "\n",
    "- `SkyCircularAperture`\n",
    "- `SkyCircularAnnulus`\n",
    "- `SkyEllipticalAperture`\n",
    "- `SkyEllipticalAnnulus`\n",
    "- `SkyRectangularAperture`\n",
    "- `SkyRectangularAnnulus`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creating an aperture object in sky coordinates is straightforward. One first uses the `SkyCoord` class to define sky coordinates and then the `SkyCircularAperture` class to define the aperture object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from photutils.aperture import SkyCircularAperture\n",
    "\n",
    "positions_sky = SkyCoord(l=[1.2, 2.3] * u.deg, b=[0.1, 0.2] * u.deg, frame='galactic')\n",
    "aperture_sky = SkyCircularAperture(positions_sky, r=4.0 * u.arcsec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to perform aperture photometry with sky-based apertures, one needs to specify a WCS transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Let's measure the photometry of sources in a Spitzer 4.5 micron image of a region of the Galactic plane. The image and the catalog are part of the `photutils.datasets` modelule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astropy.units as u\n",
    "from astropy.wcs import WCS\n",
    "from photutils.datasets import load_spitzer_catalog, load_spitzer_image\n",
    "\n",
    "spitzer_hdu = load_spitzer_image()\n",
    "spitzer_image = spitzer_hdu.data\n",
    "data = u.Quantity(spitzer_image, unit=spitzer_hdu.header['BUNIT'])  \n",
    "wcs = WCS(spitzer_hdu.header)\n",
    "catalog = load_spitzer_catalog()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spitzer_hdu.header['BUNIT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the Spitzer image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(projection=wcs)   # This initialises your axes transforming it to WCS \n",
    "ax.imshow(spitzer_image, origin='lower')\n",
    "#ax.grid(color='white', ls='solid')\n",
    "ax.set_xlabel('Galactic longitude')\n",
    "ax.set_ylabel('Galactic latitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The catalog contains (among other things) the Galactic coordinates of the sources in the image as well as the PSF-fitted fluxes from the official Spitzer data reduction. We define the apertures positions based on the existing catalog positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import SkyCoord\n",
    "\n",
    "positions = SkyCoord(catalog['l'], catalog['b'], frame='galactic')  \n",
    "aperture = SkyCircularAperture(positions, r=4.8 * u.arcsec) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the photometry in these apertures on the data. The wcs object contains the WCS transformation of the image obtained from the FITS header. It includes the coordinate frame of the image and the projection from sky to pixel coordinates. The aperture_photometry function uses the WCS information to automatically convert the apertures defined in sky coordinates into pixel coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_table = aperture_photometry(data, aperture, wcs=wcs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_table[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare the fluxes from the catalog with the aperture photometry, we need to convert the aperture sums to the same units as the catalog fluxes. \n",
    "# # The catalog fluxes are in mJy, and the aperture sums are in units of Mjy / sr. \n",
    "# We can use the pixel scale of the image to convert the aperture sums to fluxes in mJy.\n",
    "\n",
    "import astropy.units as u\n",
    "factor = (1.2 * u.arcsec) ** 2 / u.pixel\n",
    "fluxes_catalog = catalog['f4_5']  \n",
    "converted_aperture_sum = (phot_table['aperture_sum'] *\n",
    "                          factor).to(u.mJy / u.pixel)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the comparison of the photometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(fluxes_catalog, converted_aperture_sum.value)\n",
    "plt.xlabel('Spitzer catalog PSF-fit fluxes ')\n",
    "plt.ylabel('Aperture photometry fluxes')\n",
    "plt.plot([40, 100, 450], [40, 100, 450], color='black', lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-warning\"> \n",
    "**Exercise 2.1:** Read in the file `data/HelixNebula.fits` and:\n",
    "\n",
    "1. Define a SkyCoord object containing the sky coordinates of the Helix nebula.\n",
    "2. Measure the integrated flux at these sky coordinates in the image. Consider an aperture of 15 arcmin radius.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_helix = SkyCoord.(\"Helix nebula\")  # use convenience function from_name to get coordinates of the Helix nebula\n",
    "c_helix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image and header of the Helix nebula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the wcs of the Helix nebula image\n",
    "wcs_hel =  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cxpix, cypix = wcs_hel.(c_helix) # checking that the coordinates we got from the catalog match the image\n",
    "plt.imshow(img_helix, origin='lower') # display image \n",
    "plt.plot(cxpix, cypix, marker='x', color='red', markersize=3) # overlay coordinates in red "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sky aperture photometry \n",
    "aperture = \n",
    "phot_table = \n",
    "phot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Background subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `aperture_photometry()` assumes that the input data have been background-subtracted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Constant value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `bkg` is a float value or an array representing the background of the data (e.g., determined by Background2D or an external function), one can simply subtract the background from the data before aperture photometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.ones((100, 100))\n",
    "bkg = 0.1\n",
    "aperture = CircularAperture((30, 30), r=3.0)\n",
    "phot_table = aperture_photometry(data - bkg, aperture) \n",
    "print(phot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very convenient function to estimate the mean background level is `sigma_clipped_stats`. It will calculate image statistics on pixels whose intensity is below a certain threshold set by median+sigma*standard_deviation, where sigma is set by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Let's estimate the background in the Spitzer image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.stats import sigma_clipped_stats\n",
    "spit_hst = plt.hist(spitzer_image.flatten(), bins=100, log=True)\n",
    "mean_bkg, median_bkg, std_bkg = sigma_clipped_stats(spitzer_image, sigma=3.0)\n",
    "print((mean_bkg , median_bkg , std_bkg)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the potential presence of outliers, it is better to take the median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can notice that lowering the sigma threshold also lowers the mean/median background level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.stats import sigma_clipped_stats\n",
    "mean_bkg , median_bkg, std_bkg  = sigma_clipped_stats(spitzer_image, sigma=2.5)\n",
    "print((mean_bkg , median_bkg , std_bkg)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-warning\"> \n",
    "**Exercise 2.2:** Read in the file `data/M13_blue_0001.fits` and:\n",
    "\n",
    "1. Estimate uncertainties on the pixel intensities. Assume that the latter correspond to photon counts, and consider Poisson noise uncertainties.\n",
    "2. Estimate and subtract the background from the image.\n",
    "3. Measure the flux for the brightest point source in the M13 field in a 4 pixel-radius aperture, including error.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_M13 = fits.getdata('data/M13_blue_0001.fits')\n",
    "plt.imshow(img_M13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_bkg, median_bkg, std_bkg = \n",
    "median_bkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_M13_bgsub =  # Calculate background subtracted image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_M13_bgsub, vmin=0, origin='lower')   # display background subtracted image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate uncertainties on the pixels  assuming Poisson noise only \n",
    "unc_M13 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the coordinates of the brightest pixel in the background subtracted image\n",
    "max_coords = \n",
    "cy, cx = \n",
    "print(cy, cx) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the aperture on the image\n",
    "aperture = \n",
    "plt.imshow(img_M13_bgsub, vmin=0, origin='lower')\n",
    "aperture.plot(color='white', lw=1) # Plot aperture on the image \n",
    "# let's zoom in to see the brightest pixel better\n",
    "plt.xlim(450, 850)\n",
    "plt.ylim(350, 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_table =  # Calculae aperture photometry using \"Error\" as an argument  \n",
    "phot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Annular estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One often wants to also estimate the **local background** around each source using a nearby aperture or annulus aperture surrounding each source. A simple method for doing this is to use the `ApertureStats` class to compute the mean background level within the background aperture. This class can also be used to calculate more advanced statistics (e.g., a sigma-clipped median) within the background aperture (e.g., a circular annulus).\n",
    "\n",
    "**Examples:** Let's first generate a more realistic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.datasets import make_100gaussians_image\n",
    "data = make_100gaussians_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This artificial image has a known average background level of 5. Let's see if local background estimation manages to get rid of that contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we perform the photometry for three sources in a circular aperture with a radius of 5 pixels. The local background level around each source is estimated using a circular annulus of inner radius 10 pixels and outer radius 15 pixels. Let’s define the apertures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.aperture import CircularAnnulus, CircularAperture\n",
    "\n",
    "positions = [(145.1, 168.3), (84.5, 224.1), (48.3, 200.3)]\n",
    "aperture = CircularAperture(positions, r=5)\n",
    "annulus_aperture = CircularAnnulus(positions, r_in=10, r_out=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s plot the circular apertures (white) and circular annulus apertures (red) on a cutout from the image containing the three sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from astropy.visualization import simple_norm\n",
    "from photutils.aperture import CircularAnnulus, CircularAperture\n",
    "from photutils.datasets import make_100gaussians_image\n",
    "\n",
    "norm = simple_norm(data, 'sqrt', percent=99)\n",
    "plt.imshow(data, norm=norm, interpolation='nearest')\n",
    "plt.xlim(0, 170)\n",
    "plt.ylim(130, 250)\n",
    "\n",
    "ap_patches = aperture.plot(color='white', lw=2,\n",
    "                           label='Photometry aperture')\n",
    "ann_patches = annulus_aperture.plot(color='red', lw=2,\n",
    "                                    label='Background annulus')\n",
    "handles = (ap_patches[0], ann_patches[0]) # Get plot info of the first patch from each list of patches returned by the plot method -- to use in the legend\n",
    "plt.legend(loc=(0.17, 0.05), facecolor='#458989', labelcolor='white',\n",
    "           handles=handles, prop={'weight': 'bold', 'size': 11})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `ApertureStats` class to compute the mean background level within the annulus aperture at each position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.aperture import ApertureStats\n",
    "aperstats = ApertureStats(data, annulus_aperture)\n",
    "bkg_mean = aperstats.mean\n",
    "print(bkg_mean)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aperstats.std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close enough. Then it is only a matter of scaling these values by the size of the aperture used for aperture photometry, and subtracting the total expected contribution from the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_table = aperture_photometry(data, aperture)\n",
    "print(aperture.area)\n",
    "total_bkg = bkg_mean * aperture.area\n",
    "phot_bkgsub = phot_table['aperture_sum'] - total_bkg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally add the background estimates and background subtracted photometry to the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_table['total_bkg'] = total_bkg\n",
    "phot_table['aperture_sum_bkgsub'] = phot_bkgsub\n",
    "print(phot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-warning\"> \n",
    "**Supplementary exercise 2.3.:** Measure the flux for the 3 brightest point sources in the Spitzer image, after subtracting the estimated background level in a surrounding annulus. Consider appropriate values for the respective radii of the aperture and annulus. Beware of the presence of NaN values.\n",
    "\n",
    "See [02_Photometry_Exr_Spitzer.ipynb](02_Photometry_Exr_Spitzer.ipynb) for a detailed solution. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Gradient estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced tools to estimate 2D background images are also available in photutils. Most are based on a masking of the brightest sources (in a similar fashion as `sigma_clipped_stats`) followed by a smoothing (low-pass spatial filter) of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:** Let's make 100 Gaussian sources and add a smooth background gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.datasets import make_100gaussians_image\n",
    "data = make_100gaussians_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.visualization import SqrtStretch\n",
    "from astropy.visualization.mpl_normalize import ImageNormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny, nx = data.shape\n",
    "y, x = np.mgrid[:ny, :nx]\n",
    "gradient = x * y / 5000.0  # Add a gradient to the image for illustrating background estimation herebelow\n",
    "data2 = data + gradient\n",
    "\n",
    "norm = ImageNormalize(stretch=SqrtStretch())\n",
    "plt.imshow(data2, norm=norm, origin='lower', cmap='Greys_r',\n",
    "           interpolation='nearest')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by creating a `Background2D` object using a box size of 50x50 and a 3x3 median filter. We will estimate the background level in each mesh as the sigma-clipped median using an instance of `MedianBackground`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.stats import SigmaClip\n",
    "from photutils.background import Background2D, MedianBackground\n",
    "\n",
    "sigma_clip = SigmaClip(sigma=3.0)\n",
    "bkg_estimator = MedianBackground()\n",
    "bkg = Background2D(data2, (50, 50), filter_size=(3, 3),\n",
    "                   sigma_clip=sigma_clip, bkg_estimator=bkg_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot the estimated background image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bkg.background, origin='lower', cmap='Greys_r',\n",
    "           interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the background-subtracted image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data2 - bkg.background, norm=norm, origin='lower',\n",
    "           cmap='Greys_r', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional background estimation and subtraction methods available in `photutils` are described on [this page](https://photutils.readthedocs.io/en/stable/background.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-warning\"> \n",
    "**Side note:** When dedicated background images are taken close in time from the science observation, these should be used to estimate and subtract the background. A straightforward approach is to subtract the median of the stack of background images. More advanced approaches exist (some involving Principal Component Analysis - PCA). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Source identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Centroid measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`photutils.centroids` provides several functions to calculate the centroid of a single source:\n",
    "\n",
    "- `centroid_com`: Calculates the object “center of mass” from 2D image moments.\n",
    "- `centroid_quadratic`: Calculates the centroid by fitting a 2D quadratic polynomial to the data.\n",
    "- `centroid_1dg`: Calculates the centroid by fitting 1D Gaussians to the marginal x and y distributions of the data.\n",
    "- `centroid_2dg`: Calculates the centroid by fitting a 2D Gaussian to the 2D distribution of the data.\n",
    "\n",
    "Masks can be input into each of these functions to mask bad pixels. Error arrays can be input into the two Gaussian fitting methods to weight the fits.\n",
    "\n",
    "To calculate the centroids of many sources in an image, use the centroid_sources() function. This function can be used with any of the above centroiding functions or a custom user-defined centroiding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.datasets import make_4gaussians_image\n",
    "\n",
    "data_all = make_4gaussians_image()\n",
    "data = data_all[43:79, 76:104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data, origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the different methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.centroids import (centroid_1dg, centroid_2dg,\n",
    "                                 centroid_com, centroid_quadratic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinates found corresponding to the center of mass (COM) of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = centroid_com(data)\n",
    "print('x=%.2f, y=%.2f'%(x1, y1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinates found by fitting a 2D quadratic polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2, y2 = centroid_quadratic(data)\n",
    "print('x=%.2f, y=%.2f'%(x2, y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinates found by fitting a 1D Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3, y3 = centroid_1dg(data)\n",
    "print('x=%.2f, y=%.2f'%(x3, y3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinates found by fitting a 2D Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4, y4 = centroid_2dg(data)\n",
    "print('x=%.2f, y=%.2f'%(x4, y4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-warning\"> \n",
    "**Side note:** An alternative for 2D model fitting, with a wider choice of models including also 2D Moffat or 2D Airy function, are the [2D model fitting utilities in Astropy](\n",
    "https://docs.astropy.org/en/stable/modeling/fitting.html#simple-2-d-model-fitting).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results. Since they are all similar, let's include an inset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (mark_inset,\n",
    "                                                   zoomed_inset_axes)\n",
    "from photutils.centroids import (centroid_1dg, centroid_2dg,\n",
    "                                 centroid_com, centroid_quadratic)\n",
    "from photutils.datasets import make_4gaussians_image\n",
    "\n",
    "data = make_4gaussians_image()[43:79, 76:104]  # extract single object\n",
    "xycen1 = centroid_com(data)\n",
    "xycen2 = centroid_quadratic(data)\n",
    "xycen3 = centroid_1dg(data)\n",
    "xycen4 = centroid_2dg(data)\n",
    "xycens = [xycen1, xycen2, xycen3, xycen4]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 5))\n",
    "ax.imshow(data, origin='lower', interpolation='nearest')\n",
    "marker = '+'\n",
    "ms, mew = 15, 2.0\n",
    "colors = ('white', 'black', 'red', 'blue')\n",
    "for xycen, color in zip(xycens, colors):\n",
    "    plt.plot(*xycen, color=color, marker=marker, ms=ms, mew=mew)\n",
    "\n",
    "ax2 = zoomed_inset_axes(ax, zoom=6, loc=9)\n",
    "ax2.imshow(data, vmin=190, vmax=220, origin='lower',\n",
    "           interpolation='nearest')\n",
    "ms, mew = 30, 2.0\n",
    "for xycen, color in zip(xycens, colors):\n",
    "    ax2.plot(*xycen, color=color, marker=marker, ms=ms, mew=mew)\n",
    "ax2.set_xlim(13, 15)\n",
    "ax2.set_ylim(16, 18)\n",
    "mark_inset(ax, ax2, loc1=3, loc2=4, fc='none', ec='0.5')\n",
    "ax2.axes.get_xaxis().set_visible(False)\n",
    "ax2.axes.get_yaxis().set_visible(False)\n",
    "ax.set_xlim(0, data.shape[1] - 1)\n",
    "ax.set_ylim(0, data.shape[0] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-warning\"> \n",
    "**Supplementa Exercise 3.1:** Calculate the center of mass (COM) of M13 on each of the 5 images and use it to align the images.    \n",
    "\n",
    "While flexible and conceptually simple, this method is not always very accurate. We will see another method below a method using `skimage.registration` that is simpler to implement and may be your first choice.  \n",
    "\n",
    "Load all 5 images of M13 (`'data/M13_blue_000n.fits'`), then:\n",
    "\n",
    "1. Estimate and subtract the background based on the method you deem the most fitting.\n",
    "2. Align the images based on the location of their center of mass (align e.g. with respect to first one).\n",
    "3. Stack them and write a new FITS file, including a modified header.\n",
    "\n",
    "Tip: use the `shift` function from `scipy.ndimage` for subpixel shifts. Basic usage: `shifted_image = shift(image, (shift_y, shift_x))`. As usual more info in dosctrings.\n",
    "\n",
    "See [02_Photometry_alignement_M13.ipynb](02_Photometry_alignement_M13.ipynb) for an outline of the solution\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Image alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common task required before stacking astronomical images is to **align them** first. There is indeed always some jitter. An appropriate approach is to search for shifts that maximize the pair-wise cross-correlation between images of the stack.\n",
    "\n",
    "`scikit-image` has a routine to do this, based on maximimizing cross-correlation in the Fourier plane: `phase_cross_correlation`.\n",
    "\n",
    "#### Concept of the phase cross correlation: \n",
    "\n",
    "The key idea behind this method is the Fourier shift theorem. Suppose you have two images $I_2$ and $I_1$:\n",
    "\n",
    "$$\n",
    "I_2(x, y) = I_1(x - \\Delta x,\\, y - \\Delta y)\n",
    "$$\n",
    "\n",
    "where the second image is just a shifted version (by $\\Delta x, \\Delta y)$ of the first.\n",
    "\n",
    "A shift in image space becomes a **phase ramp** in Fourier space:\n",
    "\n",
    "$$\n",
    "\\mathcal{F}[I_2] = F_2(u,v)\n",
    "= F_1(u,v)\\, e^{-i (u\\,\\Delta x + v\\,\\Delta y)}\n",
    "$$\n",
    "\n",
    "So the amplitudes are the same, but the phases differ by a known linear term. \n",
    "\n",
    "That’s why doing things in Fourier space instead of direct space is powerful: *Shifts become simple phase changes*. \n",
    "\n",
    "Now, what about the cross correlation? \n",
    "In real space, the cross-correlation between two images is:\n",
    "\n",
    "$$\n",
    "C(\\Delta x, \\Delta y) = \n",
    "\\sum_{x,y} I_1(x,y)\\, I_2(x+\\Delta x, y+\\Delta y)\n",
    "$$\n",
    "\n",
    "The maximum of this function occurs at the shift $$(\\Delta x, \\Delta y)$$ (you need to repeat the calculation for a range of shifts to find that one that maximizes the the cross correlation function $C()$). \n",
    "\n",
    "In Fourier space, we can make use of the following identity:\n",
    "\n",
    "$$\n",
    "\\text{CC} = \\mathcal{F}^{-1} \\left(F_1 \\cdot F_2^*\\right)\n",
    "$$\n",
    "\n",
    "Where $F_1, F_2$ are Fourier transform of $F$\n",
    "\n",
    "*   $F_1 = \\mathcal{F}[I_1]$ \n",
    "*   $F_2 = \\mathcal{F}[I_2]$\n",
    "*   $F_2^*$ means complex conjugate\n",
    "\n",
    "If the images differ only by a shift:\n",
    "\n",
    "$$\n",
    "F_2 = F_1 \\, e^{-i(u\\Delta x + v\\Delta y)}\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "F_1 \\cdot F_2^* =\n",
    "|F_1|^2 \\, e^{+i(u\\Delta x + v\\Delta y)}\n",
    "$$\n",
    "\n",
    "Now take the inverse Fourier transform:\n",
    "\n",
    "$$\n",
    "\\mathcal{F}^{-1}[|F_1|^2 \\, e^{i(u\\Delta x + v\\Delta y)}]\n",
    "= \\delta(x - \\Delta x,\\, y - \\Delta y)\n",
    "$$\n",
    "\n",
    "This is a **delta function (\"spike\") located at the shift**!\n",
    "\n",
    "So the shift becomes a single bright peak in the correlation image and finding the shift means: *Locate the pixel of maximum value*.\n",
    "\n",
    "The method is efficient, fast, insensitive to **small** intensity variations. \n",
    "\n",
    "**Example 1:** Pixel-precision alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data\n",
    "from skimage.registration import phase_cross_correlation\n",
    "from skimage.registration._phase_cross_correlation import _upsampled_dft\n",
    "from scipy.ndimage import fourier_shift\n",
    "\n",
    "image = data.camera()\n",
    "shift = (-22.4, 13.32)\n",
    "# Let's arbitrarily create a shifted imae - The shift corresponds to the pixel offset relative to the reference image \n",
    "offset_image = fourier_shift(np.fft.fftn(image), shift)\n",
    "offset_image = np.fft.ifftn(offset_image)\n",
    "print(f'Known offset (y, x): {shift}')\n",
    "\n",
    "# pixel precision first\n",
    "shift, error, diffphase = phase_cross_correlation(image, offset_image) # with noisy images it is recommended to add: \"normalization=None\"\n",
    "print(f'Detected pixel offset (y, x): {shift} (using skimage.registration,phase_cross_correlation() method)')\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ax2 = plt.subplot(1, 3, 2, sharex=ax1, sharey=ax1)\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "\n",
    "ax1.imshow(image, cmap='gray')\n",
    "ax1.set_axis_off()\n",
    "ax1.set_title('Reference image')\n",
    "\n",
    "ax2.imshow(offset_image.real, cmap='gray')\n",
    "ax2.set_axis_off()\n",
    "ax2.set_title('Offset image')\n",
    "\n",
    "# Show the output of a cross-correlation to show what the algorithm is\n",
    "# doing behind the scene\n",
    "image_product = np.fft.fft2(image) * np.fft.fft2(offset_image).conj()  # F1 * F2* (where F2* is the complex conjugate of F2) \n",
    "cc_image = np.fft.fftshift(np.fft.ifft2(image_product))  # The cross-correlation is the inverse Fourier transform of the product of F1 and F2*. \n",
    "# The Fourier shift theorem states that a shift in the spatial domain corresponds to a phase shift in the Fourier domain. \n",
    "# Therefore, by multiplying F1 and F2*, we are effectively applying a phase shift to one of the images, which allows us to determine the amount of shift between the two images. \n",
    "# The resulting cross-correlation image will have a peak at the location corresponding to the detected shift.\n",
    "ax3.imshow(cc_image.real)\n",
    "ax3.set_axis_off()\n",
    "ax3.set_title(\"Cross-correlation\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Detected pixel offset (y, x): {shift}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sub-pixel accuracy shifts, it is recommended to increase `upsample_factor` (e.g. `upsample_factor=100` means a requested precision of 1/100 pixel; increasing this parameter also makes it slower)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For noisy images, it is recommended to set `normalization=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:** Sub-pixel precision alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subpixel precision\n",
    "shift, error, diffphase = phase_cross_correlation(image, offset_image,\n",
    "                                                  upsample_factor=100) # with noisy images it is recommended to add: \"normalization=None\"\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "ax2 = plt.subplot(1, 3, 2, sharex=ax1, sharey=ax1)\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "\n",
    "ax1.imshow(image, cmap='gray')\n",
    "ax1.set_axis_off()\n",
    "ax1.set_title('Reference image')\n",
    "\n",
    "ax2.imshow(offset_image.real, cmap='gray')\n",
    "ax2.set_axis_off()\n",
    "ax2.set_title('Offset image')\n",
    "\n",
    "# Calculate the upsampled DFT, again to show what the algorithm is doing\n",
    "# behind the scenes.  Constants correspond to calculated values in routine.\n",
    "# See source code for details.\n",
    "image_product = np.fft.fft2(image) * np.fft.fft2(offset_image).conj()  # F1 * F2* (where F2* is the complex conjugate of F2) \n",
    "cc_image = _upsampled_dft(image_product, 150, 100, (shift * 100) + 75).conj()\n",
    "ax3.imshow(cc_image.real)\n",
    "ax3.set_axis_off()\n",
    "ax3.set_title(\"Supersampled XC sub-area\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Known offset (y, x): {shift}')\n",
    "print(f'Detected subpixel offset (y, x): {shift}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_upsampled_dft?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-warning\"> \n",
    "**Exercise 3.2:** Align the 5 images of M13 based on maximum cross-correlation, and with an upsample factor of 100:\n",
    "\n",
    "1. Subtract a background of 486. from the original data (Getting a correct backround for that image is a bit tricky)\n",
    "2. Align the images with respect to the first one based on their pair-wise cross-correlation.\n",
    "3. Stack them and write a new FITS file, including a modified header.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-info\"> \n",
    "Note that in the present case the results are sensistive to the background subtraction. A general rule: **DO NOT BLINDLY TRUST the results of a tool**. Use your critical thinking to find out if the results are sensible and/or depend on parameters that seem arbitrary (or arguments of your function with default values). If results depends on parameters that seem arbitrary, this means that you will have to understand more finely how that method works ! \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.stats import SigmaClip\n",
    "from photutils.background import Background2D, MedianBackground\n",
    "\n",
    "# load images into a cube for simpler manipulation\n",
    "nim = 5\n",
    "cube_M13 = []\n",
    "for i in range(nim):\n",
    "    img_M13 = fits.getdata('data/M13_blue_{:04d}.fits'.format(i+1))\n",
    "    cube_M13.append(img_M13)\n",
    "cube_M13 = np.array(cube_M13) # convert into a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Subtract a constant background from the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M13_minus_bkg = cube_M13 -   # Subtract constant background from that image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Alignment based on cross-correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the first image of the background-subtracted cube as reference. We will align the other ones with respect to that one.\n",
    "Also we will crop to only the inner part of the image to keep the core of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROP\n",
    "y_min, y_max = 300, 700\n",
    "x_min, x_max = 450, 850\n",
    "M13_crop = M13_minus_bkg[:, y_min:y_max, x_min:x_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_image =   # Take the first cropped image of the cube as reference image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(ref_image, vmax=150, origin='lower')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can provide a mask (`reference_mask` argument) with the cross-correlation function. Let's use that feature, and only consider pixels with an intensity larger than a certain threshold.\n",
    "\n",
    "For each n-1 pairs of images (with the first one) in the cube, the mask is taking all pixel locations that are above this threshold in either or the other image of the pair.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_thr = 50 \n",
    "\n",
    "mask1 =  \n",
    "mask1[ ] = 1    # Values larger than the threshold should have value = 1 in the mask \n",
    "plt.imshow(mask1, origin='lower')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ths shifts between the reference image and the other images in the cube using phase cross-correlation method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now align the images based on these shifts, and save the cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import shift\n",
    "\n",
    "aligned_cube = [M13_minus_bkg[0]]\n",
    "for i in range(nim-1):\n",
    "    aligned_cube.append(shift(M13_minus_bkg[i+1], (shifts[i][0], shifts[i][1])))\n",
    "aligned_cube = np.array(aligned_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits.writeto('outputs/aligned_cube_cc.fits', aligned_cube, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Stack and save after modifying header**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_image = np.sum(aligned_cube, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(stacked_image, vmin=np.percentile(stacked_image,0.1), origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stacked imaged looks much better now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the original header for the first file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orihead = fits.getheader('data/M13_blue_0001.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make a copy and edit the header copy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "new_head = orihead.copy()\n",
    "new_head['COMMENT'] = 'Combination of 5 images after cross-correlation alignment'\n",
    "new_head['HISTORY'] = 'Last edited on '+now.strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make save the stacked image with new header:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'outputs/M13_blue_stacked_cc.fits'\n",
    "\n",
    "hdu = fits.PrimaryHDU(stacked_image, header=new_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdu.writeto(outfile, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Automatic source finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Photutils` includes two widely-used tools that are used to detect stars in an image, DAOFIND and IRAF’s starfind, plus a third tool that allows input of a custom user-defined kernel. More details on them are available [here](https://photutils.readthedocs.io/en/stable/reference/detection_api.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.stats import sigma_clipped_stats\n",
    "from photutils.datasets import load_star_image\n",
    "hdu = load_star_image()  \n",
    "data = hdu.data[0:401, 0:401] \n",
    "\n",
    "plt.imshow(data, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, median, std = sigma_clipped_stats(data, sigma=3.0)  \n",
    "print((mean, median, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will subtract the background and use an instance of DAOStarFinder to find the stars in the image that have FWHMs of around 3 pixels and have peaks approximately 5-sigma above the background. Running this class on the data yields an astropy Table containing the results of the star finder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.detection import DAOStarFinder\n",
    "daofind = DAOStarFinder(fwhm=3.0, threshold=5.*std)  \n",
    "sources = daofind(data - median)  \n",
    "for col in sources.colnames:  \n",
    "    if col not in ('id', 'npix'):\n",
    "        sources[col].info.format = '%.2f'  # for consistent table output\n",
    "sources.pprint(max_width=76)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s plot the image and mark the location of detected sources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from astropy.visualization import SqrtStretch\n",
    "from astropy.visualization.mpl_normalize import ImageNormalize\n",
    "from photutils.aperture import CircularAperture\n",
    "positions = np.transpose((sources['xcentroid'], sources['ycentroid']))\n",
    "apertures = CircularAperture(positions, r=4.0)\n",
    "norm = ImageNormalize(stretch=SqrtStretch())\n",
    "plt.imshow(data, cmap='Greys', origin='lower', norm=norm,\n",
    "           interpolation='nearest')\n",
    "apertures.plot(color='blue', lw=1.5, alpha=0.5); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-warning\"> \n",
    "**Exercise 3.3:** Take the image of the Horsehead nebula (`'data/HorseHead.fits'`):\n",
    "\n",
    "1. Automatically find stars in the HorseHead nebula image,\n",
    "2. Overplot circles at their location,\n",
    "3. Estimate the noise in the image assuming that we have Poisson noise \n",
    "4. Measure the aperture photometry of the stars including Poisson noise uncertainties, and after subtracting annular estimates of the background.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Automatically find stars in the HorseHead nebula image,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hh = fits.getdata('data/HorseHead.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_hh, origin='lower')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's extract basic statistics about the image\n",
    "mean, median, std = sigma_clipped_stats(img_hh, sigma=2.5)  \n",
    "print((mean, median, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's estimate the background in the upper left of the image where there is no emission from the nebula. \n",
    "bkg_est = np.median(img_hh[600:,:300])\n",
    "bkg_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from photutils.detection import DAOStarFinder\n",
    "daofind = DAOStarFinder(fwhm=6.0, threshold=3.*std)  \n",
    "sources = daofind(img_hh - bkg_est)  \n",
    "for col in sources.colnames:  \n",
    "    if col not in ('id', 'npix'):\n",
    "        sources[col].info.format = '%.2f'  # for consistent table output\n",
    "sources.pprint(max_width=76)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Overplot circles at the location of the detected stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsources = len(sources)\n",
    "nsources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [(sources['xcentroid'][i], sources['ycentroid'][i]) for i in range(nsources)]\n",
    "apertures = CircularAperture(positions, r=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "norm = simple_norm(img_hh, 'sqrt', percent=99.9)\n",
    "plt.imshow(img_hh, norm=norm, interpolation='nearest', origin='lower')\n",
    "ap_patches = apertures.plot(color='white', lw=2,\n",
    "                               label='Photometry aperture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that a FWHM of 6 pixels captures a lot of sources, but not the brightest ones. Let's try to capture more with a larger FWHM and a larger threshold.\n",
    "This trial-and error approach is guided by by looking at 1D profile of some detected stars. You may see that the bright stars are a bit fatter than the fainter ones. This fatter when brighter effect is typical of some detectors. It impacts the fwhm but also the threshold of detection, because the peak of emission is less contrasted w.r.t. the extended emission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].plot(img_hh[29, 782:842], label='faint')  # A faint star\n",
    "ax[0].plot(img_hh[743, 140:200], label='bright') # a bright star\n",
    "ax[0].legend()\n",
    "ax[1].imshow(img_hh, norm=norm, interpolation='nearest', origin='lower')\n",
    "ax[1].plot(812, 29, marker='x', color='cyan', markersize=10)\n",
    "ax[1].plot(170, 743, marker='x', color='orange', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's change the fwhm and threshold parameters to try to identify the bright sources \n",
    "from photutils.detection import DAOStarFinder\n",
    "daofind = DAOStarFinder(fwhm=15.0, threshold=5.*std)  \n",
    "bright_sources = daofind(img_hh - bkg_est)  \n",
    "for col in sources.colnames:  \n",
    "    if col not in ('id', 'npix'):\n",
    "        bright_sources[col].info.format = '%.2f'  # for consistent table output\n",
    "bright_sources.pprint()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpositions = [(bright_sources['xcentroid'][i], bright_sources['ycentroid'][i]) for i in range(len(bright_sources))]\n",
    "bapertures = CircularAperture(bpositions, r=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "norm = simple_norm(img_hh, 'sqrt', percent=99.9)\n",
    "plt.imshow(img_hh, norm=norm, interpolation='nearest', origin='lower')\n",
    "ap_patches = bapertures.plot(color='white', lw=2, label='Photometry aperture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the new criteria now captures the brightest sources. Let's see how to merge the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sources)\n",
    "#sources?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with QTables, this means we can concatenate them as we saw in Notebook 01, simply with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import vstack\n",
    "\n",
    "all_sources = vstack([sources, bright_sources])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sources.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you proceed to avoid any duplicates in this merged table? Hint: the answer is also in Notebook 06."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3** Estimate the noise in the image assuming that we have Poisson noise  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Poisson_unc = np.sqrt(img_hh)  # Poisson uncertainty is the square root of the counts in each pixel. ; Done on the image NOT background subtracted !    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Poisson_unc)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Photometry including Poisson noise uncertainties after *local* background subtraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first consider circular and annular apertures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positions = [(all_sources['xcentroid'][i], all_sources['ycentroid'][i]) for i in range(len(all_sources))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apertures = CircularAperture(all_positions, r=10)\n",
    "annuli = CircularAnnulus(all_positions, r_in=20, r_out=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "ax = plt.subplot()\n",
    "ax.imshow(img_hh, origin='lower')\n",
    "norm = simple_norm(img_hh, 'sqrt', percent=99.9)\n",
    "ax.imshow(img_hh, origin='lower', norm=norm, interpolation='nearest')\n",
    "\n",
    "ap_patches = apertures.plot(color='white', lw=2,\n",
    "                            label='Photometry aperture')\n",
    "ann_patches = annuli.plot(color='red', lw=2, label='Background annulus')\n",
    "handles = (ap_patches[i], ann_patches[i])\n",
    "plt.legend(loc=(0.17, 0.05), facecolor='#458989', labelcolor='white',\n",
    "           handles=handles, prop={'weight': 'bold', 'size': 11})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's calculate the photometry in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_all = len(all_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backgrounds\n",
    "from photutils.aperture import ApertureStats\n",
    "aperstats = ApertureStats(img_hh, annuli)\n",
    "bkg_median = aperstats.median\n",
    "bkg_median  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the first section, this time we provide the Poisson uncertainty map as `error`, when calculating the aperture photometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_tables = aperture_photometry(img_hh, apertures, error=Poisson_unc)\n",
    "total_bkg = bkg_median*apertures.area                     # scale bkg level by the area of each respective aperture \n",
    "phot_bkgsub = phot_tables['aperture_sum'] - total_bkg  # subtract bkg level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally add the background estimates and background subtracted photometry to each table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phot_tables['total_bkg'] = total_bkg\n",
    "phot_tables['aperture_sum_bkgsub'] = phot_bkgsub\n",
    "print(phot_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for more accurate estimates, one could have considered smaller/larger apertures for fainter/brighter sources. This would have implied repeating the above with different aperture sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"det1\"></a> <div class=\"alert alert-block alert-warning\"> \n",
    "**Side note:** In this lecture, we only considered the case of measuring the photometry of unresolved point sources. For **extended elliptical sources** (e.g. galaxies) it is common to use **isophotes** instead. The interested reader can refer to the [following instructions](https://photutils.readthedocs.io/en/stable/isophote.html) explaining how to use the tools available in `photutils` in such case. A more general, yet very convenient, `photutils` tool can also provide the [morphological properties](https://photutils.readthedocs.io/en/stable/morphology.html) of the intensity distribution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to top](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- This notebook is mostly based on the astropy tutorials available [here](https://learn.astropy.org/).\n",
    "- Your reference for using `astropy` should be the [online documentation](http://docs.astropy.org/en/latest). Here are links to specific documentation on [quantities](https://docs.astropy.org/en/stable/units/index.html), [constants](https://docs.astropy.org/en/stable/constants/index.html), the [`wcs` module](http://docs.astropy.org/en/stable/visualization/wcsaxes/), [FITS handling](https://docs.astropy.org/en/stable/io/fits/index.html), [`coordinates`](http://docs.astropy.org/en/stable/coordinates/index.html), [`Table` objects](http://docs.astropy.org/en/stable/table/io.html), and [modeling](https://docs.astropy.org/en/stable/modeling/index.html).\n",
    "- Photutils documentation on [aperture photometry](https://photutils.readthedocs.io/en/stable/aperture.html) and [isophotes](https://photutils.readthedocs.io/en/stable/isophote.html).\n",
    "- Scikit-image documentation on [phase cross-correlation](https://scikit-image.org/docs/stable/auto_examples/registration/plot_register_translation.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits:\n",
    "\n",
    "If you use Astropy directly—or as a dependency to another package—for your work, please remember to include the following acknowledgment at the end of papers:\n",
    "\n",
    "*This research made use of Astropy, a community-developed core Python package for Astronomy (Astropy Collaboration, 2013).*\n",
    "\n",
    "Where the astropy paper is 2013, A&A, 558, 33 http://adsabs.harvard.edu//abs/2013A%26A...558A..33A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go back to top](#Table-of-contents)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py_SPAT0086_py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
